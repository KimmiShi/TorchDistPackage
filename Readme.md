# TorchDistPackage

TorchDistPackage provides some easy-to-use modules and tools for Distributed Training in PyTorch.

It is under construction. Welcome to use and contribute.

# Simple DDP Module in PyTorch

code and example: [NaiveDdp](./ddp)

Highlights:

- Easy to understand and debug, no c++ api used.
- overlaps grad reduce with compute like torch ddp

# Toolkit

- [slurm init](./slurm_dist_init/)

